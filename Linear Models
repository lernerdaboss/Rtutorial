#Linear Models
#Least square to find best fit line
#ggplot(data, aes(y, x))+
  #geom_point() + geom_smooth(method="lm, se=FALSE)
ggplot(bdims, aes(x=wgt, y=hgt))+
  geom_point()+
  geom_smooth(method="lm", se =)

#residuals are realisation of noise term
#given n observations, minimise sum of the squared distances

bdims_summary <- summary(bdims)
bdims_summary <- data.frame(bdims_summary)
# Add slope and intercept
sd_wgt <- sd(bdims$wgt)
sd_hgt <- sd(bdims$hgt)
mean_wgt <- mean(bdims$wgt)
mean_hgt <- mean(bdims$hgt)
bdims %>%
  mutate(slope = cor(wgt, hgt) * sd_wgt / sd_hgt, 
         intercept = mean_wgt - slope * mean_hgt)

mod <- (lm(uclaNew ~ amazNew, data = textbooks))
lm(bdims$wgt ~ bdims$hgt)

textbooks %>%
  mutate(amazNew_cents = amazNew*100) %>%
  lm(uclaNew ~ amazNew_cents, data = .)

# Add the line to the scatterplot
ggplot(data = bdims, aes(x = hgt, y = wgt)) + 
  geom_point() + 
  geom_abline(data = coefs, 
              aes(intercept = `(Intercept)`, slope = hgt),  
              color = "dodgerblue")
summary(mod)
fitted.values(mod)
residuals(mod)

library(broom)
augment(mod)


#MAKING PREDICTIONS LINEAR MODEL
augment(mod) %>%
  arrange(desc(.resid)) %>%
  head()

textbooks %>%
  filter(uclaNew == 197)

#predict(lm, newdata (a dataframe))
new_data <- data.frame(textbooks$amazNew)
predict(mod, newdata = new_data, interval = "confidence")


#Veryfying Assumptions
#plot(lmvalues, lmresiduals)  ---- No pattern ie: independent
#qqnorm(lmresiduals) and identical normal therefore a qqplot

# shop_data has been loaded in your workspace

# Add a plot: sales as a function of inventory. Is linearity plausible?
plot(sales ~ sq_ft, shop_data)
plot(sales ~ size_dist, shop_data)
plot(sales ~ inv, shop_data)

# Build a linear model for net sales based on all other variables: lm_shop
lm_shop <- lm(sales ~ ., data = shop_data)

# Summarize lm_shop
summary(lm_shop)

# shop_data, shop_new and lm_shop have been loaded in your workspace

# Plot the residuals in function of your fitted observations
plot(lm_shop$fitted.values,  lm_shop$residuals,
     xlab = "Fitted values", ylab = "Residuals")

# Make a Q-Q plot of your residual quantiles
qqnorm(lm_shop$residuals, ylab = "Residual Quantiles")

# Summarize your model, are there any irrelevant predictors?
summary(lm_shop)

# Predict the net sales based on shop_new.
predict(lm_shop, shop_new)

# choco_data has been loaded in your workspace

# Add a plot:  energy/100g as function of total size. Linearity plausible?
plot(energy ~ protein, choco_data)
plot(energy ~ fat, choco_data)
plot(energy ~ size, choco_data)

# Build a linear model for the energy based on all other variables: lm_choco
lm_choco <- lm(energy ~ ., data = choco_data)

# Plot the residuals in function of your fitted observations
plot(lm_choco$fitted.values,  lm_choco$residuals,
     xlab = "Fitted values", ylab = "Residuals")

# Make a Q-Q plot of your residual quantiles
qqnorm(lm_choco$residuals, ylab = "Residual Quantiles")

# Summarize lm_choco
summary(lm_choco)

############################################################################

# kang_nose is pre-loaded in your workspace

# Build model and make plot
lm_kang <- lm(nose_length ~ nose_width, data=kang_nose)
plot(kang_nose, xlab = "nose width", ylab = "nose length")
abline(lm_kang$coefficients, col = "red")

# Apply predict() to lm_kang: nose_length_est
nose_length_est <- predict(lm_kang)

# Calculate difference between the predicted and the true values: res
res <- kang_nose$nose_length - nose_length_est

# Calculate RMSE, assign it to rmse and print it
rmse <- sqrt(mean(res ^ 2))
print(rmse)

# kang_nose, lm_kang and res are already loaded in your workspace

# Calculate the residual sum of squares: ss_res
ss_res <- sum(res ^ 2)

# Determine the total sum of squares: ss_tot
ss_tot <- sum((kang_nose$nose_length - mean(kang_nose$nose_length)) ^ 2)

# Calculate R-squared and assign it to r_sq. Also print it.
r_sq <- 1 - ss_res / ss_tot
r_sq

# Apply summary() to lm_kang
summary(lm_kang)


# world_bank_train and cgdp_afg is available for you to work with

# Plot urb_pop as function of cgdp
plot(world_bank_train,
     xlab = "GDP per Capita",
     ylab = "Percentage of urban population")

# Set up a linear model between the two variables: lm_wb
lm_wb <- lm(urb_pop ~ cgdp, data = world_bank_train)

# Add a red regression line to your scatter plot
abline(lm_wb$coefficients, col = "red")

# Summarize lm_wb and select R-squared
summary(lm_wb)$r.squared

# Predict the urban population of afghanistan based on cgdp_afg
predict(lm_wb, cgdp_afg)



############################################################

#MULTIVARIABLE LINEAR REGRESSION
#my_lm <- lm(sales ~ ads + competitors + ... + , data = )
#summary(my_lm)$adj.r.squared


#######################################################
#CLASSIFICATION TREES
# The train and test set are loaded into your workspace.

# Set random seed. Don't remove this line.
set.seed(1)

# Load the rpart, rattle, rpart.plot and RColorBrewer package
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

# Fill in the ___, build a tree model: tree
tree <- rpart(Survived ~ ., train, method = "class")

# Draw the decision tree
fancyRpartPlot(tree)

# The train and test set are loaded into your workspace.

# Code from previous exercise
set.seed(1)
library(rpart)
tree <- rpart(Survived ~ ., train, method = "class")

# Predict the values of the test set: pred
pred <- predict(tree, test, type = "class")

# Construct the confusion matrix: conf
conf <- table(test$Survived, pred)

# Print out the accuracy
print(sum(diag(conf)) / sum(conf))

# All packages are pre-loaded, as is the data

# Calculation of a complex tree
set.seed(1)
tree <- rpart(Survived ~ ., train, method = "class", control = rpart.control(cp=0.00001))

# Draw the complex tree
fancyRpartPlot(tree)

# Prune the tree: pruned
pruned <- prune(tree, cp = 0.01)

# Draw pruned
fancyRpartPlot(pruned)








#ASSESSING MODEL FIT
ggplot(data = textbooks, aes(x = amazNew, y = uclaNew))+
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
#fits well
ggplot(data = possum, aes(x=tailL, y = totalL))+
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

#sums of squared deviations "ln" SSE

mod_possum <- lm(totalL ~ tailL, data = possum)
mod_possum %>%
  augment()%>%
  summarize(SSE = sum(.resid^2),
            SSE_also = (n() - 1) * var(.resid))

#ROOT MEAN SQUARED ERROR RMSE - std of residuals
RMSE <- sqrt(sum(residuals(mod)^2) / df.residual(mod))


#COMPARING MODEL FITS
#Calculating R^2 - how close data is fitted to regression line
#R_squared = 1 - var_e / var_y
#in summary function
bdims_tidy <- augment(mod)
bdims_tidy %>%
  summarize(var_y = var(uclaNew), var_e =var(.resid)) %>%
  mutate(R_squares = 1 - var_e / var_y)


#Removing outliers
new_dataframe <- with_outlier %>%
  filter(!(x > 60 & y < 30))

########################################################
#Confusion Matrix

# The titanic dataset is already loaded into your workspace

# Set random seed. Don't remove this line
set.seed(1)

# Have a look at the structure of titanic
str(titanic)

# A decision tree classification model is built on the data
tree <- rpart(Survived ~ ., data = titanic, method = "class")

# Use the predict() method to make predictions, assign to pred
pred <- predict(tree, titanic, type = "class")

# Use the table() method to make the confusion matrix
table(titanic$Survived, pred)

# The confusion matrix is available in your workspace as conf

# Assign TP, FN, FP and TN using conf
TP <- conf[1, 1]
FN <- conf[1, 2]
FP <- conf[2, 1]
TN <- conf[2, 2]

# Calculate the accuracy: acc
acc <- (TP + TN) / (TP + FN + FP + TN)
acc

# Calculate and print out the precision: prec
prec <- TP / (TP + FP)
prec

# Calculate and print out the recall: rec
rec <- TP / (TP + FN)
rec

# The air dataset is already loaded into your workspace.

# Take a look at the structure of air
str(air)

# Inspect your colleague's code to build the model
fit <- lm(dec ~ freq + angle + ch_length, data = air)

# Use the model to predict for all values: pred
pred <- predict(fit)

# Use air$dec and pred to calculate the RMSE 
rmse <- sqrt((1/nrow(air)) * sum( (air$dec - pred) ^ 2))

# Print out rmse
rmse

# Your colleague's more complex model
fit2 <- lm(dec ~ freq + angle + ch_length + velocity + thickness, data = air)

# Use the model to predict for all values: pred2
pred2 <- predict(fit2)

# Calculate rmse2
rmse2 <- sqrt(sum( (air$dec - pred2) ^ 2) / nrow(air))

# Print out rmse2
rmse2


################################################
#KMEANS CLUSTERING
# The seeds dataset is already loaded into your workspace

# Set random seed. Don't remove this line
set.seed(1)

# Explore the structure of the dataset
str(seeds)

# Group the seeds in three clusters
km_seeds <- kmeans(seeds, 3)

# Color the points in the plot based on the clusters
plot(length ~ compactness, data = seeds, col = km_seeds$cluster)

# Print out the ratio of the WSS to the BSS
km_seeds$tot.withinss / km_seeds$betweenss




#########################################################
#SPLITTING DATA
# The titanic dataset is already loaded into your workspace

# Set random seed. Don't remove this line.
set.seed(1)

# Shuffle the dataset; build train and test
n <- nrow(titanic)
shuffled <- titanic[sample(n),]
train <- shuffled[1:round(0.7 * n),]
test <- shuffled[(round(0.7 * n) + 1):n,]

# Fill in the model that has been learned.
tree <- rpart(Survived ~ ., train, method = "class")

# Predict the outcome on the test set with tree: pred
pred <- predict(tree, test, type = "class")

# Calculate the confusion matrix: conf
conf <- table(test$Survived, pred)

# Print this confusion matrix
print(conf)

# Set random seed. Don't remove this line.
set.seed(1)

# The shuffled dataset is already loaded into your workspace

# Initialize the accs vector
accs <- rep(0,6)

for (i in 1:6) {
  # These indices indicate the interval of the test set
  indices <- (((i-1) * round((1/6)*nrow(shuffled))) + 1):((i*round((1/6) * nrow(shuffled))))
  
  # Exclude them from the train set
  train <- shuffled[-indices,]
  
  # Include them in the test set
  test <- shuffled[indices,]
  
  # A model is learned using each training set
  tree <- rpart(Survived ~ ., train, method = "class")
  
  # Make a prediction on the test set using tree
  pred <- predict(tree, test, type = "class")
  
  # Assign the confusion matrix to conf
  conf <- table(test$Survived, pred)
  
  # Assign the accuracy of this model to the ith index in accs
  accs[i] <- sum(diag(conf))/sum(conf)
}

# Print out the mean of accs
mean(accs)

#############################################
#Bias and Variance
# The all-knowing classifier that has been learned for you
# You should change the code of the classifier, simplifying it
spam_classifier <- function(x){
  prediction <- rep(NA, length(x))
  prediction[x > 4] <- 1
  prediction[x <= 4] <- 0
  return(factor(prediction, levels = c("1","0")))
}

# conf_small and acc_small have been calculated for you
conf_small <- table(emails_small$spam, spam_classifier(emails_small$avg_capital_seq))
acc_small <- sum(diag(conf_small)) / sum(conf_small)
acc_small

# Apply spam_classifier to emails_full and calculate the confusion matrix: conf_full
conf_full <- table(emails_full$spam, spam_classifier(emails_full$avg_capital_seq))

# Calculate acc_full
acc_full <- sum(diag(conf_full)) / sum(conf_full)

# Print acc_full
acc_full


################################################
#ROC CURVE
# A tree model, tree, and test set, test, are loaded into your workspace

# Code of previous exercise
set.seed(1)
tree <- rpart(income ~ ., train, method = "class")
probs <- predict(tree, test, type = "prob")[,2]

# Load the ROCR library
library(ROCR)

# Make a prediction object: pred
pred <- prediction(probs, test$income)

# Make a performance object: perf
perf <- performance(pred, "tpr", "fpr")

# Plot this curve
plot(perf)

# Make a performance object: perf
perf <- performance(pred, "auc")

# Print out the AUC
print(perf@y.values[[1]])





#Chapter2
#tidyr package
#gather(df, my_key, my_val, -Column)
#spread(df, my_ky, my_val)
#separate(df, col, into ie:c("year", "month"), sep = "-")
#unite(df, col ie:newname, col1, col2, sep = "-")

#Chapter3
#character: "12314"
#numerics: 233, NaN, Inf
#intefer: 4L, 422L
#factor: factor("hey"), factor(8) ----- Categorical data
#logical: TRUE, FALSE, NA
#class(variable)

#Type conversions: as.character(2016), as.numeric(TRUE) = 1, as.factor("something") = something

#DATETIME --- LUBRIDATE package
library(lubridate)
ymd("2015-08-25")
ymd("2015 August 25")
#="2015-08-25"
#ALSO mdy, hms, ymd_hms

#STRING MANIPULATIONS
library(stringr)
str_trim("   this is a test    ")
#="this is a test"
str_pad("25453", width = 10, side="left", pad="0")
#=0000025453
friends <- c("Sarah", "alice", "peter")
str_detect(friends, "Sarah")
str_replace(friends, "Sarah", "Alex")



#Missing and Special Values
#Finding missing values
df <- data.frame(A=c(22, NA, 3),
                 B=c(NA, 3, 91),
                 C=c(2,33, 34),
                 D=c(44, 1, 13))
is.na(df)
any(is.na(df))
sum(is.na(df))
#=2
summary(df)
complete.cases(df)

#keeping rows with no missing values:
df[complete.cases(df),]
#another way remove rows with NAs
na.omit(df)
#which(is.na(data))



#Spotting outliers and common errors
set.seed(10)
x <- c(rnorm(50, mean=15,sd=5), -55, 200, 55, 0)
boxplot(x, horizontal = TRUE)
#removing obvious errors



#Putting it all together



#tidying weather data
weather1 <- gather(weather, measure, my_val, X1:X31, na.rm = TRUE)
#removing first column
without_X <- weather1[, -1]
weather3 <- spread(without_X, measure, my_val)



#DPLYR manipulation
#select --- remove columns from table
#filter --- remove rows
#arrange --- re-orders dataset
#mutate adds more columns
#summarise --- summary statistics
